**Tokenization** is to separate a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords.
